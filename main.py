from fastapi import FastAPI, UploadFile, File, Form, HTTPException
from fastapi.responses import JSONResponse
from typing import List
import uvicorn
import pandas as pd

# Import the ResumeAnalyzer and LinkedinScraper classes from core_functions.py
from core_functions import ResumeAnalyzer, LinkedinScraper

app = FastAPI()


@app.post("/job-recommendations")
async def job_recommendations(
    name: str = Form(...),
    age: int = Form(...),
    gender: str = Form(...),
    experience: int = Form(...),
    job_type: str = Form(...),  # Comma separated string
    location: str = Form(...),
    skills: str = Form(...),
    openai_api_key: str = Form(...),
    resume: UploadFile = File(...)
):
    """
    Endpoint to get job recommendations based on resume and user details.
    Expects user details and a resume PDF. Returns the resume summary and job recommendations generated by OpenAI.
    """
    if resume.content_type != "application/pdf":
        raise HTTPException(status_code=400, detail="Resume must be a PDF file.")
    try:
        # Process the resume into text chunks using the provided pdf file
        chunks = ResumeAnalyzer.pdf_to_chunks(resume.file)
        
        # Generate resume summary prompt and summary
        summary_prompt_text = ResumeAnalyzer.summary_prompt(query_with_chunks=chunks)
        summary = ResumeAnalyzer.openai(openai_api_key, chunks, analyze=summary_prompt_text)
        
        # Build user details dictionary as required by the recommendation prompt
        user_details = {
            "name": name,
            "age": age,
            "gender": gender,
            "experience": experience,
            "job_type": [j.strip() for j in job_type.split(",") if j.strip()],
            "location": location,
            "skills": skills
        }
        
        # Generate job recommendation prompt and recommendations
        job_rec_prompt = ResumeAnalyzer.job_recommendation_prompt(user_details, summary)
        recommendations = ResumeAnalyzer.openai(openai_api_key, chunks, analyze=job_rec_prompt)
        
        return {"resume_summary": summary, "job_recommendations": recommendations}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/linkedin-jobs")
async def linkedin_jobs(
    job_titles: str = Form(...),  # Comma separated job titles
    job_location: str = Form(...),
    job_count: int = Form(...)
):
    """
    Endpoint to scrape LinkedIn jobs based on job title(s), location, and number of jobs to fetch.
    Returns a list of job postings with company name, job title, location, website URL, and job description.
    """
    try:
        # Convert job_titles string to list
        job_titles_list = [job.strip() for job in job_titles.split(",") if job.strip()]
        
        # Initialize Selenium webdriver
        driver = LinkedinScraper.webdriver_setup()
        
        # Build the LinkedIn jobs URL
        link = LinkedinScraper.build_url(job_titles_list, job_location)
        
        # Open the link and scroll down to load jobs
        LinkedinScraper.link_open_scrolldown(driver, link, job_count)
        
        # Scrape company data from the page
        df = LinkedinScraper.scrap_company_data(driver, job_titles_list, job_location)
        
        # Scrape job descriptions from individual job pages
        df_final = LinkedinScraper.scrap_job_description(driver, df, job_count)
        
        driver.quit()
        
        # Convert the DataFrame to a list of dictionaries to return as JSON
        jobs = df_final.to_dict(orient="records")
        return {"linkedin_jobs": jobs}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8001) 